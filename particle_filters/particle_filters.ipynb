{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Table of Contents](./table_of_contents.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "from numpy.random import uniform, random, seed\n",
    "from numpy.linalg import norm\n",
    "from numpy.random import randn\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gimme_some_pi(N=300, display_visuals=False):\n",
    "    pts = uniform(-1, 1, (N, 2))\n",
    "\n",
    "    # distance from (0,0) \n",
    "    dist = np.linalg.norm(pts, axis=1)\n",
    "    in_circle = dist <= 1\n",
    "\n",
    "    pts_in_circle = np.count_nonzero(in_circle)\n",
    "    # surface is Ï€ since radius is 1.0\n",
    "    pi = 4 * (pts_in_circle / N)\n",
    "\n",
    "    # plot results\n",
    "    if display_visuals:\n",
    "        plt.scatter(pts[in_circle,0], pts[in_circle,1], \n",
    "                    marker=',', edgecolor='k', s=1)\n",
    "        plt.scatter(pts[~in_circle,0], pts[~in_circle,1], \n",
    "                    marker=',', edgecolor='r', s=1)\n",
    "        plt.axis('equal')\n",
    "    \n",
    "    return pi, np.pi-pi\n",
    "#     print('mean pi(N={})= {:.4f}'.format(N, pi))\n",
    "#     print('err  pi(N={})= {:.4f}'.format(N, np.pi-pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi, pi_error = gimme_some_pi(N=5000, display_visuals=True)\n",
    "\n",
    "print (\"pi={}  pi_error={}\".format(np.round(pi, 3), np.round(pi_error, 3)))\n",
    "\n",
    "N_range = np.arange(1, 200000, 1000)\n",
    "errs = [np.abs(gimme_some_pi(N=N)[1]) for N in N_range]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.yscale('log')\n",
    "plt.plot(N_range, errs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap Particle Filter Algorithm\n",
    "\n",
    "0. **Randomly generate a bunch of particles**\n",
    "<img src=\"img/initialize_from_prior_step.png\"  alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "    Particles can have position, heading, and/or whatever other state variable you need to estimate. Each has a weight (probability) indicating how likely it matches the actual state of the system. Initialize each with the same weight.\n",
    " \n",
    "\n",
    "\n",
    "1. **Predict next state of the particles**\n",
    "<img src=\"img/predict_step.png\"  alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    " Move the particles based on how you predict the real system is behaving.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. **Update the weights**\n",
    "<img src=\"img/update_step.png\"  alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "  Update the weighting of the particles based on the measurement. Particles that closely match the measurements are weighted higher than particles that don't.\n",
    "  \n",
    "  \n",
    "3. **Resample**\n",
    "<img src=\"img/resampling_step.png\"  alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "  Discard highly improbable particles and replace them with copies of the more probable particles.\n",
    "\n",
    "\n",
    "4. **Optionally, Compute Estimate**\n",
    "\n",
    "  Optionally, compute weighted mean and covariance of the set of particles to get a state estimate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. **Randomly generate a bunch of particles**\n",
    "<img src=\"img/initialize_from_prior_step.png\"  alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "    Particles can have position, heading, and/or whatever other state variable you need to estimate. Each has a weight (probability) indicating how likely it matches the actual state of the system. Initialize each with the same weight.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_uniform_particles(x_range, y_range, hdg_range, N):\n",
    "    particles = np.empty((N, 3))\n",
    "    particles[:, 0] = uniform(x_range[0], x_range[1], size=N)\n",
    "    particles[:, 1] = uniform(y_range[0], y_range[1], size=N)\n",
    "    particles[:, 2] = uniform(hdg_range[0], hdg_range[1], size=N)\n",
    "    particles[:, 2] %= 2 * np.pi\n",
    "    return particles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_uniform_particles((0,1), (0,1), (0, np.pi*2), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Predict next state of the particles**\n",
    "<img src=\"img/predict_step.png\"  alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    " Move the particles based on how you predict the real system is behaving.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(particles, polar_velocity, polar_velocity_stddev, dt=1.):\n",
    "    \"\"\" move according to control input polar_velocity (heading change, speed)\n",
    "    with noise polar_velocity_stddev a.k.a. Q (stddev of heading change, stddev of speed)`\"\"\"\n",
    "\n",
    "    N = len(particles)\n",
    "    # update heading of particles all at once\n",
    "    particles[:, 2] += polar_velocity[0] + (randn(N) * polar_velocity_stddev[0])\n",
    "    particles[:, 2] %= 2 * np.pi\n",
    "\n",
    "    # move in the (noisy) commanded direction\n",
    "    dist = (polar_velocity[1] * dt) + (randn(N) * polar_velocity_stddev[1])\n",
    "    particles[:, 0] += np.cos(particles[:, 2]) * dist\n",
    "    particles[:, 1] += np.sin(particles[:, 2]) * dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Update the weights**\n",
    "<img src=\"img/update_step.png\"  alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "  Update the weighting of the particles based on the measurement. Particles that closely match the measurements are weighted higher than particles that don't. Normalizing the weights so they sum to one turns them into a probability distribution. \n",
    "  Measurements here = range measurements. \n",
    "  Likelihood of (noisy!) Range measurements given the current particle-estimated robot position.\n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(particles, particle_weights, sensor_measurements, sensor_noise_stddev, true_landmarks):\n",
    "    for i, landmark in enumerate(landmarks):\n",
    "        # distance as per (given) the particle state (i.e. the x in p(y|x))\n",
    "        estimated_distance = np.linalg.norm(particles[:, 0:2] - landmark, axis=1)\n",
    "        # measurement model is normal law centered on estimated state (i.e. distances/ranges from the robot to landmarks)\n",
    "        # all we know about the robot is estimated from the particles \n",
    "        particle_weights *= scipy.stats.norm(estimated_distance, sensor_noise_stddev).pdf(sensor_measurements[i])\n",
    "\n",
    "    particle_weights += 1.e-300      # avoid round-off to zero\n",
    "    particle_weights /= sum(particle_weights) # normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Resample**\n",
    "<img src=\"img/resampling_step.png\"  alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "  Discard highly improbable particles and replace them with copies of the more probable particles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the literature this part of the algorithm is called *Sequential Importance Sampling*, or SIS. The equation for the weights is called the *importance density*. I will give these theoretical underpinnings in a following section. For now I hope that this makes intuitive sense. If we weight the particles according to how well they match the measurements they are probably a good sample for the probability distribution of the system after incorporating the measurements. Theory proves this is so. The weights are the *likelihood* in Bayes theorem. Different problems will need to tackle this step in slightly different ways but this is the general idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_resample(particles, weights):\n",
    "    N = len(particles)\n",
    "    cumulative_sum = np.cumsum(weights)\n",
    "    cumulative_sum[-1] = 1. # avoid round-off error\n",
    "    indexes = np.searchsorted(cumulative_sum, random(N))\n",
    "\n",
    "    # resample according to indexes\n",
    "    particles[:] = particles[indexes]\n",
    "    weights.fill(1.0 / N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Optionally, Compute Estimate**\n",
    "\n",
    "  Optionally, compute weighted mean and covariance of the set of particles to get a state estimate.\n",
    "\n",
    "$$ \\mu = \\frac{1}{N}\\sum\\limits_{i=1}^N w^ix^i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate(particles, weights):\n",
    "    \"\"\"returns mean and variance of the weighted particles\"\"\"\n",
    "\n",
    "    pos = particles[:, 0:2]\n",
    "    mean = np.average(pos, weights=weights, axis=0)\n",
    "    var  = np.average((pos - mean)**2, weights=weights, axis=0)\n",
    "    return mean, var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's piece those steps together for a simple end-to-end Particle Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the filter we need to create the particles and the landmarks. We then execute a loop, successively calling `predict`, `update`, `resampling`, and then computing the new state estimate with `estimate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true, known, surveyed position of landmarks. \n",
    "# The robot uses a colocated sensor to measure ranges to the landmarks. \n",
    "landmarks = np.array([[-1, 2], [5, 10], [12,14], [18,21]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sensor_measurements(landmarks, sensor_position, sensor_std_dev):\n",
    "    return (norm(landmarks - sensor_position, axis=1) + \n",
    "              (randn(len(landmarks)) * sensor_std_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_particle_filter(N_particles, num_iterations=3, sensor_std_dev=0.1, \n",
    "                        do_plot=False, plot_particles=False,\n",
    "                        x_lim=(0, 20), y_lim=(0, 20), \n",
    "                        does_resampling=True,\n",
    "                        is_initialized=None):\n",
    "    \n",
    "    # create particles and weights\n",
    "    particles = create_uniform_particles(x_lim, y_lim, (0, 2.*np.pi), N_particles)\n",
    "    weights = np.ones(N_particles) / N_particles\n",
    "\n",
    "    rover_position = np.array([0., 0.])\n",
    "    for x in range(num_iterations):\n",
    "        # move true robot/sensor position\n",
    "        rover_position += (1, 1)\n",
    "\n",
    "        # sensor-measured distances from robot to each landmark. Truth + some sensor noise\n",
    "        sensor_measurements = get_sensor_measurements(landmarks=landmarks,\n",
    "                                                      sensor_position=rover_position,\n",
    "                                                      sensor_std_dev=sensor_std_dev)\n",
    "\n",
    "        # move particles diagonally forward to (x+1, x+1), no angular velocity\n",
    "        predict(particles,\n",
    "                polar_velocity=(0.00, np.sqrt(2.)),\n",
    "                polar_velocity_stddev=(.2, .05))\n",
    "        \n",
    "        # incorporate measurements info. does not alter the particle positions, only alters the weights\n",
    "        update(particles=particles,\n",
    "               particle_weights=weights,\n",
    "               sensor_measurements=sensor_measurements,\n",
    "               sensor_noise_stddev=sensor_std_dev, \n",
    "               true_landmarks=landmarks)\n",
    "\n",
    "        # resampling\n",
    "        if does_resampling:\n",
    "            simple_resample(particles=particles, weights=weights)\n",
    "        \n",
    "        # get an estimate at current time step   \n",
    "        mu, var = estimate(particles, weights)\n",
    "\n",
    "    print('final position error, variance:\\n\\t', mu - np.array([num_iterations, num_iterations]), var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# seed(2)  # seed(2) + N_particles==1000 will give you bad estimates but not other seeds ==> demo\n",
    "\n",
    "run_particle_filter(N_particles=40000,\n",
    "                    plot_particles=False,\n",
    "                    num_iterations=20,\n",
    "                    does_resampling=True,\n",
    "                    x_lim=(0,8), y_lim=(0,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_particle_filter(N_particles, num_iterations=3, sensor_std_dev=0.1, \n",
    "                        do_plot=False, plot_particles=False,\n",
    "                        x_lim=(0, 20), y_lim=(0, 20), \n",
    "                        does_resampling=True,\n",
    "                        is_initialized=None,\n",
    "                        fig_=None):\n",
    "\n",
    "    \n",
    "    # create particles and weights\n",
    "    particles = create_uniform_particles(x_lim, y_lim, (0, 2.*np.pi), N_particles)\n",
    "    weights = np.ones(N_particles) / N_particles\n",
    "    if plot_particles:\n",
    "        alpha = .20\n",
    "        if N > 5000:\n",
    "            alpha *= np.sqrt(5000)/np.sqrt(N)           \n",
    "        plt.scatter(particles[:, 0], particles[:, 1], \n",
    "                    alpha=alpha, color='g')\n",
    "    \n",
    "    rover_position_estimates = []\n",
    "    rover_position = np.array([0., 0.])\n",
    "    for x in range(num_iterations):\n",
    "        # move true robot/sensor position\n",
    "        rover_position += (1, 1)\n",
    "\n",
    "        # sensor-measured distances from robot to each landmark. Truth + some sensor noise\n",
    "        sensor_measurements = get_sensor_measurements(landmarks=landmarks,\n",
    "                                                      sensor_position=rover_position,\n",
    "                                                      sensor_std_dev=sensor_std_dev)\n",
    "\n",
    "        # move particles diagonally forward to (x+1, x+1), no angular velocity\n",
    "        predict(particles,\n",
    "                polar_velocity=(0.00, np.sqrt(2.)),\n",
    "                polar_velocity_stddev=(.2, .05))\n",
    "        \n",
    "        # incorporate measurements info. does not alter the particle positions, only alters the weights\n",
    "        update(particles=particles,\n",
    "               particle_weights=weights,\n",
    "               sensor_measurements=sensor_measurements,\n",
    "               sensor_noise_stddev=sensor_std_dev, \n",
    "               true_landmarks=landmarks)\n",
    "        \n",
    "        if does_resampling:\n",
    "            simple_resample(particles=particles, weights=weights)\n",
    "        # get an estimate at current time step   \n",
    "        mu, var = estimate(particles, weights)\n",
    "        rover_position_estimates.append(mu)\n",
    "\n",
    "        if plot_particles:\n",
    "            plt.scatter(particles[:, 0], particles[:, 1], \n",
    "                        color='k', marker=',', s=1)\n",
    "        p1 = plt.scatter(rover_position[0], rover_position[1], marker='+',\n",
    "                         color='k', s=180, lw=3)\n",
    "        p2 = plt.scatter(mu[0], mu[1], marker='s', color='r')\n",
    "    \n",
    "    rover_position_estimates = np.array(rover_position_estimates)\n",
    "    plt.plot(rover_position_estimates[:, 0], rover_position_estimates[:, 1])\n",
    "    plt.legend([p1, p2], ['Actual', 'Particle Filter'], loc=4, numpoints=1)\n",
    "    plt.xlim(*x_lim)\n",
    "    plt.ylim(*y_lim)\n",
    "    fig_.canvas.draw()\n",
    "#     display.display(fig_)\n",
    "\n",
    "    return fig_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "N_particles, num_iterations = 40000, 20\n",
    "\n",
    "the_fig, ax = plt.subplots(1,1, figsize=(16 * 1.0, 9 * 1.0))\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "\n",
    "for one_seed in range(20):\n",
    "    plt.clf()\n",
    "    fig = run_particle_filter(N_particles=N_particles,\n",
    "                    do_plot=True,\n",
    "                    plot_particles=False,\n",
    "                    num_iterations=num_iterations,\n",
    "                    does_resampling=True,\n",
    "                    x_lim=(0, num_iterations+1), y_lim=(0, num_iterations+1),\n",
    "                             fig_=the_fig)\n",
    "    \n",
    "    sleep(1.5)\n",
    "\n",
    "# keep last iteration in the plot window\n",
    "the_fig.set_figheight(9*0.5)\n",
    "the_fig.set_figwidth(16*0.5)\n",
    "the_fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/resampling_examples.png\"  alt=\"Drawing\" style=\"width: 600px;\"/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
